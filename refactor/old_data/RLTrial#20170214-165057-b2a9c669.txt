choice_noise = 0.05
n_intervals = 100
neuron_type = 'default'
N_state_action = 500
D = 5
direct = False
seed = 1
T_interval = 0.2
alpha = 0.3
dt = 0.001
backend = 'nengo'

stay_prob = array([[ 0.5       ,  0.59259259],
       [ 1.        ,  0.33333333]])
rewards = [True, False, True, True, True, False, True, True, True, True, False, True, True, True, True, True, False, False, False, True, True, False, False, True, False, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, False, True, False]
history = [(0, 'SA'), (0, 'SA'), (0, 'SA'), (0, 'SA'), (0, 'SA'), (0, 'SA'), (0, 'SB'), (0, 'SB'), (1, 'SB'), (0, 'SA'), (1, 'SB'), (0, 'SA'), (1, 'SB'), (0, 'SA'), (0, 'SA'), (1, 'SB'), (0, 'SB'), (0, 'SA'), (0, 'SB'), (0, 'SB'), (1, 'SB'), (1, 'SB'), (0, 'SA'), (0, 'SA'), (0, 'SA'), (1, 'SB'), (0, 'SA'), (1, 'SA'), (1, 'SA'), (0, 'SB'), (1, 'SB'), (0, 'SA'), (0, 'SB'), (1, 'SB'), (1, 'SB'), (0, 'SB'), (0, 'SA'), (1, 'SA'), (0, 'SA'), (0, 'SA'), (0, 'SA'), (0, 'SA'), (0, 'SB'), (0, 'SA'), (0, 'SB'), (0, 'SA'), (0, 'SA'), (0, 'SA'), (1, 'SA'), (1, 'SA')]
